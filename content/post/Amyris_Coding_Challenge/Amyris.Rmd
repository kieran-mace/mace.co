---
title: "HTS Challenge"
author: Kieran Mace
date: '2018-03-15'
output:
  html_document:
    df_print: paged
---

## Challenge:

* Try to pick out strains youâ€™d promote to the next round of screening.  
* This is meant to be an interactive sport, email with any questions.

 

### Data:

Data are 60 96-well plates are filled with:

1. 88 possibly improved strains (Standard Well), 
2. 4 process controls (Process Control), and 
3. 4 parent strains (Parent Strain) from which the Standard Well strains derive. 

* The 60 plates are processed and measured on 3 different robots: Bender, Terminator, and C3P0.
* On each robot, the 20 plates are loaded onto a stacker and are fed into the analyzer one plate at a time. It takes about 3 minutes to measure all wells of a plate.

### Goal:

The goal of our HTS group is to take data like this and pick out standard well strains that have been improved over their parent.
 
* Pick out improved strains you would promote to the next round of testing.
    + Most Standard Well strains are not improved.
    + There is noise in the assay.
    + What would you promote if you could screen 20, 60, or 180 strains in another round of screening?
* Comment about features you see in the data and how you found them and what you would do with them.
* Send us a few graphical visualizations of the raw or transformed data that you found helpful.

We are less interested in your list of hits as we are in the process by which you arrived at that list and the ways you might have visualized the data.  Feel free to use any tools that you think you might want to use.  If you want the data in a different format, let us know.

## Data Exploration:

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(magrittr)
library(skimr)
theme_set(theme_minimal())
wells = readxl::read_xlsx('Sample T1 data.xlsx')
```

Lets just calculate the well number:
```{r}
wells %<>% 
  mutate(well_number = counter %% 96,
         well_number = if_else(well_number == 0, 96, well_number)) %>%
  filter(well_type != 'Process Control')
```


Since we want to correct for any confounding variables that are not strain related, we will first explore the `Parent Strain` and `Process Control` wells, which __should__ only have stochastic variation in the `value` variable. 

```{r}
wells %>% 
  filter(well_type %in% c('Parent Strain')) -> control_wells
```

```{r}
control_wells %>% 
  ggplot(aes(x = value, fill = robot)) +
  geom_density(alpha = 0.4) +
  facet_wrap(~well_type, ncol = 1)
```

From my experiance with plate readers, I suspect that the robot, and measurement times may affect the value, Lets investigate these hypotheses graphically.



Lets look to see how the parent strain performs as a function of stacker position and robot.

```{r}
control_wells %>% 
  ggplot(aes(y=value, x = stacker_position, color = robot)) + 
  geom_point() + 
  geom_smooth() +
  facet_wrap(~well_type, ncol = 1, scales="free_y")
  
```

It seems that `stacker_position` has a very strong effect! We will need to model this effect in order to correct for it. Since the line is relativly striaght and smooth, a simple linear model, or spline model should accurately account for the majority of the stacker position effect. 

It is unclear as to whether the `stacker_position` effect depends on robot. Visually it seems that the effect is independent on the `robot`, but we can check this with a multifactor anova. 

```{r}
control_wells %>% 
  {lm(value ~ stacker_position + stacker_position:robot, data = .)} -> product_model
anova(product_model)
```
Considering the effect of the robot, we cannot regect the null hypothesis, that stacker_position opperates independently of robot, so we can model the stacker effect over all robots.

## Correcting for confounding effects

We will build a spline model from the `Parent Strain` data, and verify its fit from the `Process Control` data.

```{r}
control_wells %>% 
  filter(well_type == 'Parent Strain') %>% 
  ggplot(aes(x = stacker_position, 
             y = value)) + 
  geom_point() + 
  geom_smooth()

control_wells %>% 
  filter(well_type == 'Parent Strain') %>% 
  {loess(value~stacker_position, data = .)} -> stacker_effect_model
```

Lets now subtract that signal and see if the Parent Strain now performs consitantly across samples.

```{r}
control_wells$value_corrected = control_wells$value - predict(stacker_effect_model, newdata = control_wells)

ggplot(control_wells, aes(x = stacker_position, 
                          y = value_corrected, 
                          color = robot)) +
  geom_point() +
  facet_wrap(~well_type, ncol = 1)
```

The fit seems to do a good job of accounting for the effect, even in the `Process Control` data that was not explicitly modeled. 

## Picking most promising strains:

First we will calculate corrected values for the `Standard Well`s.

```{r}
wells$value_corrected = wells$value - predict(stacker_effect_model, newdata = wells)
```
Although we have corrected for the dominant confounding signal, we should still check to see if there is a `robot` effect. Lets start that process by calulating some summary statistics on the corrected values. 

```{r}
wells %>% 
  filter(well_type == 'Parent Strain') %>%
  group_by(robot) %>% 
  select(value_corrected) %>%
  skim() %>% 
  skimr::kable()
```
It seems that the robot still has an effect, however it is inclear how to account for the effect. There seems to be a mean effect, as well as some differences in the variance of each measurement. One approach is to assume nothing about the underlying distribution of each `robots` measurement, but instead split the data on each robot, and analyze them separately. To visualize the differences, lets take a look at the value distribution of the `Parent Strain` for each `robot`. 
```{r}
wells %>% 
  filter(well_type %in% c('Parent Strain')) %>%
  ggplot(aes(x = value_corrected, fill = robot)) +
  geom_density(alpha = .4) +
  facet_wrap(~well_type, ncol = 1)
```
The distributions here seem to have different variances, however this is hard to act on with only 80 `Parent Strain` measurements per robot. We know very little about the underlying distributions (we can not assume normailty), but we do have 80 measurements per robot, which is relativly large. 

Null Hypothesis: a strain is no different from its parent strain, and therefore has a value sampled from the same distribution as the parent strain.

Alturnative Hypothesis: The strain is different from the parent strain

Since we make no assumpitions about the normaility of the `Parent Strain` distribution, we can use a wilcox rank sum test. 

```{r}

get_pval = function(value, robot){
  value_distribution = control_wells %>% 
    filter(robot == robot & well_type == 'Parent Strain') %>% 
    pull(value_corrected)
  test = wilcox.test(value_distribution, value, correct=FALSE)
  return(test$p.value)
}

wells %<>% 
  arrange(desc(value_corrected)) %>%
  mutate(p_value = Vectorize(get_pval)(value_corrected, robot),
         rank = rank(desc(value_corrected)))
  


```


The following plot shows that althogh each robot has a different distribution, the magnitude of the `value_corrected` seems to have a robot indepentant relationship with p_value, meaning (as we calculated before) that the robot's distribution has very little confounding effects.

```{r}
wells %>% ggplot(aes(x = value_corrected, y = -log(p_value), color = robot)) + geom_point(alpha  = .3)

```

## Picking strains to promote.

In order to pick strains to promote, I would pick the strains with the highest corrected values. According the the wilcox test, although I cannot be statistically certain that all of these strains are definitely better than the parent strain, I can say that they have about a 8.5% chance of being the same as the parent strain.

```{r}
wells %>% filter(well_type == 'Standard Well') %>% select(strain, value_corrected, p_value, rank)
```


One comment I might have going forward in this process is to investigate the investment required to perform replicate measurements for each strain. This of course would come down to the amount of resources required to do so, as well as the expected number of strains that need to be tested in order to find a "sucessor" to the parent strain. 

For example, if we expect that one in every 100 strains will be better than the parent strain, and each strain we move to the next stage will require lots of investment, then we may prefer to make 3 measurements per strain at the initial stages, to weed out bad strains before we invest in developing them further.

## Using Prediction Interval:

```{r}
wells %>% filter(well_type == 'Parent Strain') %>% {lm(value ~ stacker_position + well_number, data = .)} -> parent_model

wells = cbind(wells, predict(parent_model, wells, interval = 'p', level = .95))
```
```{r}
ggplot(wells, aes(x = fit, y = value,shape = well_type,  color = value > upr | value < lwr)) + geom_point() + geom_abline(slope = 1, intercept = 0)
```

```{r}
wells %>% filter(value > upr)
```
 There are 43 Strains that I am more than 95% percent confident are better than the parent strain.

So we did a stistical prediction interval so that we could have a sense of the confidence interval. Lets take a look at the number of parent strains that exceed the prediction interval:
```{r}
wells %>% filter(well_type == 'Parent Strain') %>% mutate(outlier = value > upr | value < lwr) %>% pull(outlier) %>% table() %>% prop.table()

wells %>% filter(well_type == 'Standard Well') %>% mutate(outlier = value > upr | value < lwr) %>% pull(outlier) %>% table() %>% prop.table()
```

So as expected we have a 5% false discovery rate for the parent strains. Interestingly, we have a 10% of candidate strains that do not conform to the parent strain model. Meaning to me that out selected strains are about half falsely labled and half worth promoting to the next stage. 

Lets compare the two methods:

```{r}
ggplot(wells, aes(x = value_corrected, y = value - upr, color = well_type)) + geom_point()
```


## Downstream analysis

One thing I would love to know is the cost to Amgen to promote and test a strain, as well as the expected margins of improvment a given percentage would yeild. With this information, one could use something like Lagrangian constrained optimizaiton to maximize the productivity of the strain improvment process. 
